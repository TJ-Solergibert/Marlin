
defaults:
  - Llama3-base
  - _self_

run:
  name: Llama3-70B

trainer:
  num_nodes: 8

exp_manager:
  checkpoint_callback_params:
    model_parallel_size: 32

model:
  # Parallelism config
  tensor_model_parallel_size: 4
  sequence_parallel: True
  pipeline_model_parallel_size: 8
  virtual_pipeline_model_parallel_size: 5 # NOTE(tj.solergibert) Num layers = 80, (80 / pipeline_model_parallel_size) / 2 = 5 Layers per virtual pipeline stage
  context_parallel_size: 1

  # Model architecture
  num_layers: 80
  hidden_size: 8192
  ffn_hidden_size: 28672
  num_attention_heads: 64
  init_method_std: 0.008944

  tokenizer:
    type: meta-llama/Meta-Llama-3-70B

  # Pipeline parallel communication overlap
  overlap_p2p_comm: True
  batch_p2p_comm: False
  # Pipeline bubble overlap
  defer_embedding_wgrad_compute: True
  wgrad_deferral_limit: 22

  # Optimizer
  # optim:
  #Â   align_param_gather: True # NOTE(tj.solergibert) Requires virtual_pipeline_model_parallel_size > 1 & MCore distributed Adam
